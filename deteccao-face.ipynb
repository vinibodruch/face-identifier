{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in ./face_detection/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: tensorflow in ./face_detection/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: pillow in ./face_detection/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: scikit-learn in ./face_detection/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.3 in ./face_detection/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (1.67.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: packaging in ./face_detection/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: setuptools in ./face_detection/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./face_detection/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./face_detection/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./face_detection/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./face_detection/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./face_detection/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: optree in ./face_detection/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: namex in ./face_detection/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: rich in ./face_detection/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./face_detection/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./face_detection/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./face_detection/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./face_detection/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./face_detection/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./face_detection/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./face_detection/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./face_detection/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./face_detection/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./face_detection/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./face_detection/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 22:15:46.395577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-28 22:15:46.397009: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-28 22:15:46.401212: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-28 22:15:46.414068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-28 22:15:46.435551: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-28 22:15:46.442403: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-28 22:15:46.459522: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-28 22:15:47.677713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python tensorflow pillow scikit-learn\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Caminhos\n",
    "data_dir = './faces'  # Pasta com os dados de treino\n",
    "output_dir = './faces_detectadas'\n",
    "\n",
    "# Parâmetros\n",
    "img_width, img_height = 150, 150\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Cria o diretório de saída se não existir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Inicializa contador de imagem\n",
    "img_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Varre cada subpasta dentro do diretório de dados\n",
    "for person_name in os.listdir(data_dir):\n",
    "    person_path = os.path.join(data_dir, person_name)\n",
    "\n",
    "    # Verifica se é um diretório (para garantir que é uma pasta de pessoa)\n",
    "    if os.path.isdir(person_path):\n",
    "        # Itera sobre cada imagem dentro da subpasta da pessoa\n",
    "        for img_name in os.listdir(person_path):\n",
    "            img_path = os.path.join(person_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is not None:\n",
    "                # Converte a imagem para escala de cinza (requisito para o Haar Cascade)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Detecta faces na imagem\n",
    "                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "                # Para cada face detectada, recorta e salva com o nome formatado\n",
    "                for (x, y, w, h) in faces:\n",
    "                    face_img = img[y:y+h, x:x+w]\n",
    "                    resized_face = cv2.resize(face_img, (img_width, img_height))\n",
    "\n",
    "                    # Define o nome do arquivo de saída\n",
    "                    output_filename = f'img{img_counter}_{person_name}.jpg'\n",
    "                    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "                    # Salva a face detectada\n",
    "                    cv2.imwrite(output_path, resized_face)\n",
    "                    print(f'Face detectada salva em: {output_path}')\n",
    "\n",
    "                    # Incrementa o contador de imagens\n",
    "                    img_counter += 1\n",
    "            else:\n",
    "                print(f'Erro ao carregar imagem: {img_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Data augmentation e preparação do gerador de dados de treino para as faces detectadas\u001b[39;00m\n\u001b[1;32m      5\u001b[0m datagen_faces \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m      7\u001b[0m train_generator_faces \u001b[38;5;241m=\u001b[39m datagen_faces\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m      8\u001b[0m     data_dir_faces,\n\u001b[1;32m      9\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(img_width, img_height),\n\u001b[0;32m---> 10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mbatch_size\u001b[49m,\n\u001b[1;32m     11\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m validation_generator_faces \u001b[38;5;241m=\u001b[39m datagen_faces\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     16\u001b[0m     data_dir_faces,\n\u001b[1;32m     17\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(img_width, img_height),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Impressão de informações do gerador\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Atualiza o diretório de dados para as faces detectadas\n",
    "data_dir_faces = './faces_detectadas'\n",
    "\n",
    "# Data augmentation e preparação do gerador de dados de treino para as faces detectadas\n",
    "datagen_faces = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "train_generator_faces = datagen_faces.flow_from_directory(\n",
    "    data_dir_faces,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator_faces = datagen_faces.flow_from_directory(\n",
    "    data_dir_faces,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Impressão de informações do gerador\n",
    "print(\"Classes detectadas: \", train_generator_faces.class_indices)\n",
    "print(\"Número de imagens de treinamento: \", train_generator_faces.samples)\n",
    "print(\"Número de imagens de validação: \", validation_generator_faces.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinicius/Documents/Aulas/ProcessamentoImagem/face_detection/face_detection/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/vinicius/Documents/Aulas/ProcessamentoImagem/face_detection/face_detection/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 382ms/step - accuracy: 0.1488 - loss: 2.8602\n",
      "Epoch 2/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302ms/step - accuracy: 0.2279 - loss: 2.1583\n",
      "Epoch 3/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 312ms/step - accuracy: 0.2420 - loss: 1.6546\n",
      "Epoch 4/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288ms/step - accuracy: 0.0541 - loss: 1.6006\n",
      "Epoch 5/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - accuracy: 0.2337 - loss: 1.6051\n",
      "Epoch 6/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step - accuracy: 0.3954 - loss: 1.5784\n",
      "Epoch 7/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step - accuracy: 0.3476 - loss: 1.5873\n",
      "Epoch 8/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278ms/step - accuracy: 0.3990 - loss: 1.5712\n",
      "Epoch 9/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 281ms/step - accuracy: 0.4517 - loss: 1.5196\n",
      "Epoch 10/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - accuracy: 0.2904 - loss: 1.5041\n",
      "Epoch 11/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - accuracy: 0.2279 - loss: 1.5383\n",
      "Epoch 12/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - accuracy: 0.5990 - loss: 1.4216\n",
      "Epoch 13/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - accuracy: 0.4240 - loss: 1.3306\n",
      "Epoch 14/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288ms/step - accuracy: 0.3662 - loss: 1.2753\n",
      "Epoch 15/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - accuracy: 0.3070 - loss: 1.2349\n",
      "Epoch 16/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step - accuracy: 0.5697 - loss: 1.0207\n",
      "Epoch 17/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269ms/step - accuracy: 0.5635 - loss: 1.5469\n",
      "Epoch 18/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - accuracy: 0.4365 - loss: 1.3492\n",
      "Epoch 19/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - accuracy: 0.4933 - loss: 1.2453\n",
      "Epoch 20/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 449ms/step - accuracy: 0.5573 - loss: 1.3756\n",
      "Epoch 21/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step - accuracy: 0.4167 - loss: 1.1085\n",
      "Epoch 22/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 326ms/step - accuracy: 0.5562 - loss: 1.1682\n",
      "Epoch 23/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - accuracy: 0.6269 - loss: 1.0016\n",
      "Epoch 24/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - accuracy: 0.6005 - loss: 0.9153\n",
      "Epoch 25/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347ms/step - accuracy: 0.5327 - loss: 0.9880\n",
      "Epoch 26/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318ms/step - accuracy: 0.5635 - loss: 1.0725\n",
      "Epoch 27/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - accuracy: 0.6306 - loss: 0.8466\n",
      "Epoch 28/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325ms/step - accuracy: 0.6655 - loss: 0.7910\n",
      "Epoch 29/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299ms/step - accuracy: 0.7154 - loss: 0.7162\n",
      "Epoch 30/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319ms/step - accuracy: 0.7576 - loss: 0.7956\n",
      "Epoch 31/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - accuracy: 0.8731 - loss: 0.5947\n",
      "Epoch 32/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302ms/step - accuracy: 0.6738 - loss: 0.7708\n",
      "Epoch 33/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293ms/step - accuracy: 0.6930 - loss: 0.6303\n",
      "Epoch 34/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - accuracy: 0.7779 - loss: 0.6705\n",
      "Epoch 35/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286ms/step - accuracy: 0.4250 - loss: 1.0823\n",
      "Epoch 36/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274ms/step - accuracy: 0.7820 - loss: 0.6382\n",
      "Epoch 37/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 326ms/step - accuracy: 0.7029 - loss: 0.5546\n",
      "Epoch 38/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - accuracy: 0.8611 - loss: 0.5205\n",
      "Epoch 39/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305ms/step - accuracy: 0.8538 - loss: 0.4594\n",
      "Epoch 40/40\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333ms/step - accuracy: 0.7388 - loss: 0.5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado e salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Caminhos e Parâmetros\n",
    "data_dir = './faces_detectadas'\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 10\n",
    "\n",
    "# Carregar e organizar os dados manualmente\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # Carrega a imagem e redimensiona\n",
    "        img_path = os.path.join(data_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (img_width, img_height))\n",
    "        \n",
    "        # Normaliza a imagem\n",
    "        img = img / 255.0\n",
    "        images.append(img)\n",
    "        \n",
    "        # Extrai o rótulo do nome do arquivo (assumindo o formato 'img{numero}_{nome}.jpg')\n",
    "        label = filename.split('_')[1].split('.')[0]\n",
    "        labels.append(label)\n",
    "\n",
    "# Converte as listas em arrays numpy\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Codificação das Labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "labels_categorical = to_categorical(labels_encoded, num_classes)\n",
    "\n",
    "# Gerador de dados para aumento de dados\n",
    "data_gen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
    "train_generator = data_gen.flow(images, labels_categorical, batch_size=batch_size)\n",
    "\n",
    "# Definição do Modelo\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinamento do Modelo\n",
    "model.fit(train_generator, epochs=40)\n",
    "\n",
    "# Salvar o Modelo Treinado\n",
    "model.save('face_recognition_model.h5')\n",
    "\n",
    "print(\"Modelo treinado e salvo com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Acurácia: nan\n",
      "F1-Score (Weighted): nan\n",
      "Matriz de Confusão:\n",
      "[]\n",
      "\n",
      "Relatório de Classificação:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinicius/Documents/Aulas/ProcessamentoImagem/face_detection/face_detection/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/vinicius/Documents/Aulas/ProcessamentoImagem/face_detection/face_detection/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 0, does not match size of target_names, 31. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Relatório de Classificação Completo (opcional)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRelatório de Classificação:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Aulas/ProcessamentoImagem/face_detection/face_detection/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Aulas/ProcessamentoImagem/face_detection/face_detection/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2648\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2642\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2643\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2644\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[1;32m   2645\u001b[0m             )\n\u001b[1;32m   2646\u001b[0m         )\n\u001b[1;32m   2647\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2648\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[1;32m   2652\u001b[0m         )\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2654\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 0, does not match size of target_names, 31. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# Caminhos\n",
    "test_dir = './faces/lebron'  # Diretório das imagens de teste\n",
    "model_path = './face_recognition_model.h5'\n",
    "\n",
    "# Parâmetros da imagem\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# Carrega o modelo treinado\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Recria o LabelEncoder usado no treinamento\n",
    "labels = [filename.split('_')[1].split('.')[0] for filename in os.listdir('./faces_detectadas')]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Listas para armazenar rótulos reais e predições\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "\n",
    "# Laço para processar cada imagem de teste\n",
    "for filename in os.listdir(test_dir):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        # Caminho completo da imagem\n",
    "        img_path = os.path.join(test_dir, filename)\n",
    "        \n",
    "        # Carrega a imagem original para detecção de rosto\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Converte para RGB para processamento\n",
    "        faces = face_cascade.detectMultiScale(img_rgb, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "        # Laço para cada rosto detectado\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extrai e redimensiona a face detectada\n",
    "            face = img_rgb[y:y+h, x:x+w]\n",
    "            face_resized = cv2.resize(face, (img_width, img_height)) / 255.0\n",
    "            face_resized = np.expand_dims(face_resized, axis=0)\n",
    "            \n",
    "            # Realiza a predição\n",
    "            prediction = model.predict(face_resized)\n",
    "            predicted_class = np.argmax(prediction, axis=1)\n",
    "            predicted_label = label_encoder.inverse_transform(predicted_class)[0]\n",
    "            \n",
    "            # Desenha o retângulo e o rótulo na imagem\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Salva a imagem rotulada\n",
    "        # output_path = os.path.join(output_dir, f\"labeled_{filename}\")\n",
    "        # cv2.imwrite(output_path, img)\n",
    "        cv2.imshow(filename, img)\n",
    "        cv2.waitKey(990)  # Aguarda uma tecla para exibir a próxima imagem\n",
    "        cv2.destroyAllWindows()  \n",
    "        # print(f\"Imagem {filename} processada e salva em {output_path}\")\n",
    "\n",
    "# Converte os rótulos reais para inteiros correspondentes\n",
    "y_true_encoded = label_encoder.transform(y_true)\n",
    "y_pred_encoded = label_encoder.transform(y_pred)\n",
    "\n",
    "# Calcula a acurácia\n",
    "accuracy = accuracy_score(y_true_encoded, y_pred_encoded)\n",
    "print(f\"Acurácia: {accuracy:.2f}\")\n",
    "\n",
    "# Calcula o F1-Score\n",
    "f1 = f1_score(y_true_encoded, y_pred_encoded, average='weighted')\n",
    "print(f\"F1-Score (Weighted): {f1:.2f}\")\n",
    "\n",
    "# Calcula e exibe a Matriz de Confusão\n",
    "conf_matrix = confusion_matrix(y_true_encoded, y_pred_encoded)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Relatório de Classificação Completo (opcional)\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_true_encoded, y_pred_encoded, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# # Carrega o modelo treinado\n",
    "# model = load_model('./face_recognition_model.h5')\n",
    "\n",
    "# # Dicionário de classes\n",
    "# class_dict = {v: k for k, v in train_generator.class_indices.items()}\n",
    "# df = cv2.CascadeClassifier('classifier.xml')\n",
    "\n",
    "# input_dir = './faces/elvis'\n",
    "\n",
    "# # Processa cada imagem na pasta 'faces'\n",
    "# for filename in os.listdir(input_dir):\n",
    "#     if filename.endswith('.jpg'):\n",
    "#         file_path = os.path.join(input_dir, filename)\n",
    "#         i = cv2.imread(file_path)\n",
    "#         iPB = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#         # Executa a detecção de faces\n",
    "#         faces = df.detectMultiScale(iPB, scaleFactor=1.05, minNeighbors=7, minSize=(30,30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "#         # Salva cada face detectada na nova pasta com rótulo\n",
    "#         for j, (x, y, w, h) in enumerate(faces):\n",
    "#             face = i[y:y+h, x:x+w]\n",
    "#             face_resized = cv2.resize(face, (img_width, img_height))\n",
    "#             face_normalized = face_resized / 255.0\n",
    "#             face_reshaped = np.reshape(face_normalized, (1, img_width, img_height, 3))\n",
    "#             prediction = model.predict(face_reshaped)\n",
    "#             label = class_dict[np.argmax(prediction)]\n",
    "#             face_filename = os.path.join(output_dir, f'{os.path.splitext(filename)[0]}_{label}_{j}.jpg')\n",
    "#             cv2.imwrite(face_filename, face)\n",
    "#             cv2.rectangle(i, (x, y), (x + w, y + h), (0, 255, 255), 7)\n",
    "#             cv2.putText(i, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "#         # Exibe a imagem com as faces detectadas e rotuladas\n",
    "#         cv2.imshow(f'{filename} - {len(faces)} face(s) encontrada(s)', i)\n",
    "#         cv2.waitKey(9900)\n",
    "#         cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
